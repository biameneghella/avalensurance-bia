{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff82d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_PATH : /Users/augusto/Library/Mobile Documents/com~apple~CloudDocs/git/avalensurance-bia/data/full_warehouse_merged.csv\n",
      "OUT_PATH: /Users/augusto/Library/Mobile Documents/com~apple~CloudDocs/git/avalensurance-bia/data/full_warehouse_imputed_critical.csv\n",
      "Dataset loaded: (64800, 59)\n",
      "Critical columns to impute: ['bmi', 'systolic_bp', 'diastolic_bp', 'ldl', 'hba1c', 'income']\n",
      "[1/6] bmi: filled=0, remaining_nans=0, time=0.0s\n",
      "[2/6] systolic_bp: filled=0, remaining_nans=0, time=0.0s\n",
      "[3/6] diastolic_bp: filled=0, remaining_nans=0, time=0.0s\n",
      "[4/6] ldl: filled=0, remaining_nans=0, time=0.0s\n",
      "[5/6] hba1c: filled=0, remaining_nans=0, time=0.0s\n",
      "[6/6] income: filled=0, remaining_nans=0, time=0.0s\n",
      "Total time: 0.0074310302734375\n",
      "Saved imputed critical file to: /Users/augusto/Library/Mobile Documents/com~apple~CloudDocs/git/avalensurance-bia/data/full_warehouse_imputed_critical.csv\n",
      "Comparison: {'n_rows': 7200, 'mean_old': np.float64(2880.7365777864647), 'mean_new': np.float64(1929.2105288611112), 'mean_diff': np.float64(-951.5260489253537), 'median_diff': np.float64(-45.12425710122568), 'std_diff': np.float64(2874.035175693014), 'pct_increase': np.float64(0.4847222222222222), 'top5_increases': [{'person_id': 65708, 'annual_medical_cost_old': 428.4936748397115, 'annual_medical_cost_new': 2057.7175, 'diff': 1629.2238251602887}, {'person_id': 24847, 'annual_medical_cost_old': 435.5504866972855, 'annual_medical_cost_new': 1979.0956, 'diff': 1543.5451133027145}, {'person_id': 88838, 'annual_medical_cost_old': 413.1089241524717, 'annual_medical_cost_new': 1925.9836, 'diff': 1512.8746758475284}, {'person_id': 60837, 'annual_medical_cost_old': 416.2835049527072, 'annual_medical_cost_new': 1922.3328, 'diff': 1506.0492950472926}, {'person_id': 82360, 'annual_medical_cost_old': 428.6453360054871, 'annual_medical_cost_new': 1929.8716, 'diff': 1501.226263994513}], 'top5_decreases': [{'person_id': 52069, 'annual_medical_cost_old': 38488.901926987026, 'annual_medical_cost_new': 1938.8508, 'diff': -36550.051126987026}, {'person_id': 17154, 'annual_medical_cost_old': 32585.885802512392, 'annual_medical_cost_new': 1936.684, 'diff': -30649.20180251239}, {'person_id': 51611, 'annual_medical_cost_old': 32022.049171020713, 'annual_medical_cost_new': 1938.1489, 'diff': -30083.900271020713}, {'person_id': 11111, 'annual_medical_cost_old': 31201.146807227324, 'annual_medical_cost_new': 1936.6063, 'diff': -29264.540507227324}, {'person_id': 64005, 'annual_medical_cost_old': 29670.713425716556, 'annual_medical_cost_new': 1937.6488, 'diff': -27733.064625716557}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>filled</th>\n",
       "      <th>remaining_nans</th>\n",
       "      <th>time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bmi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>systolic_bp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>diastolic_bp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ldl</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hba1c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>income</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            col  filled  remaining_nans    time_s\n",
       "0           bmi       0               0  0.001854\n",
       "1   systolic_bp       0               0  0.002844\n",
       "2  diastolic_bp       0               0  0.000091\n",
       "3           ldl       0               0  0.000082\n",
       "4         hba1c       0               0  0.000079\n",
       "5        income       0               0  0.000083"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== Imputação nested apenas nas variáveis críticas + comparação de submissions ==========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE = \"/Users/augusto/Library/Mobile Documents/com~apple~CloudDocs/git/avalensurance-bia/data\"\n",
    "\n",
    "DF_PATH  = f\"{BASE}/full_warehouse_merged.csv\"\n",
    "OUT_PATH = f\"{BASE}/full_warehouse_imputed_critical.csv\"\n",
    "SUB1_PATH = f\"{BASE}/submission.csv\"\n",
    "SUB2_PATH = f\"{BASE}/submission2.csv\"\n",
    "\n",
    "print(\"DF_PATH :\", DF_PATH)\n",
    "print(\"OUT_PATH:\", OUT_PATH)\n",
    "\n",
    "\n",
    "# ---------- Carregar df ----------\n",
    "df = pd.read_csv(DF_PATH)\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "\n",
    "# ---------- Configurações ----------\n",
    "PROTECTED_COLS = {\"annual_medical_cost\", \"person_id\"}\n",
    "MAX_LEVELS = 3\n",
    "N_BINS = 10\n",
    "TOP_K_PREDICTORS = 10\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def is_numeric(s: pd.Series) -> bool:\n",
    "    return pd.api.types.is_numeric_dtype(s)\n",
    "\n",
    "def compute_corr_abs(df):\n",
    "    num = df.select_dtypes(include=[np.number]).copy()\n",
    "    return num.corr().abs()\n",
    "\n",
    "def get_top_predictors_for(col, corr_matrix, topk=10):\n",
    "    if col not in corr_matrix.columns:\n",
    "        return []\n",
    "    s = corr_matrix[col].drop(labels=[col], errors=\"ignore\").sort_values(ascending=False)\n",
    "    return s.index.tolist()[:topk]\n",
    "\n",
    "def create_binned_cols_local(df_local, cols, n_bins=10):\n",
    "    binned = {}\n",
    "    for c in cols:\n",
    "        if c not in df_local.columns:\n",
    "            continue\n",
    "        if is_numeric(df_local[c]):\n",
    "            try:\n",
    "                df_local[c + \"_bin\"] = pd.qcut(df_local[c].rank(method=\"first\"), q=n_bins, labels=False, duplicates=\"drop\")\n",
    "            except Exception:\n",
    "                try:\n",
    "                    df_local[c + \"_bin\"] = pd.cut(df_local[c], bins=n_bins, labels=False)\n",
    "                except Exception:\n",
    "                    df_local[c + \"_bin\"] = np.nan\n",
    "            binned[c] = c + \"_bin\"\n",
    "        else:\n",
    "            df_local[c + \"_bin\"] = df_local[c].astype(str).fillna(\"missing\")\n",
    "            binned[c] = c + \"_bin\"\n",
    "    return binned\n",
    "\n",
    "def impute_col_nested_medians_local(df_local, col, corr_matrix, max_levels=3, n_bins=10, top_k_predictors=8):\n",
    "    if col not in df_local.columns or col in PROTECTED_COLS:\n",
    "        return 0\n",
    "    n_before = df_local[col].isna().sum()\n",
    "    if n_before == 0:\n",
    "        return 0\n",
    "    top_preds = get_top_predictors_for(col, corr_matrix, topk=top_k_predictors)\n",
    "    if not top_preds:\n",
    "        top_preds = [c for c in df_local.select_dtypes(include=[np.number]).columns if c != col][:top_k_predictors]\n",
    "    top_preds = [p for p in top_preds if p != col and p in df_local.columns]\n",
    "    binned_map = create_binned_cols_local(df_local, top_preds, n_bins=n_bins)\n",
    "    for level in range(1, max_levels+1):\n",
    "        if level > len(top_preds): break\n",
    "        keys = top_preds[:level]\n",
    "        key_bins = [binned_map.get(k) for k in keys if binned_map.get(k) is not None]\n",
    "        if not key_bins: continue\n",
    "        grouped = df_local.groupby(key_bins)[col].median()\n",
    "        missing_idx = df_local[df_local[col].isna()].index\n",
    "        med_values = []\n",
    "        for idx in missing_idx:\n",
    "            row = df_local.loc[idx]\n",
    "            try:\n",
    "                key = tuple(row[k] for k in key_bins) if len(key_bins) > 1 else row[key_bins[0]]\n",
    "                m = grouped.loc[key] if len(key_bins) > 1 else grouped.loc[key]\n",
    "            except Exception:\n",
    "                m = np.nan\n",
    "            med_values.append(m)\n",
    "        med_series = pd.Series(med_values, index=missing_idx)\n",
    "        to_fill = med_series[med_series.notna()]\n",
    "        if not to_fill.empty:\n",
    "            df_local.loc[to_fill.index, col] = to_fill.values\n",
    "        if df_local[col].isna().sum() == 0:\n",
    "            break\n",
    "    # fallback by reference percentile\n",
    "    if df_local[col].isna().sum() > 0:\n",
    "        ref = None\n",
    "        for p in top_preds:\n",
    "            if is_numeric(df_local[p]):\n",
    "                ref = p; break\n",
    "        if ref is None:\n",
    "            numeric_cols = [c for c in df_local.select_dtypes(include=[np.number]).columns if c != col]\n",
    "            ref = numeric_cols[0] if numeric_cols else None\n",
    "        if ref is not None:\n",
    "            try:\n",
    "                df_local[\"__ref_bin__\"] = pd.qcut(df_local[ref].rank(method=\"first\"), q=10, labels=False, duplicates=\"drop\")\n",
    "                grouped_ref = df_local.groupby(\"__ref_bin__\")[col].median()\n",
    "                missing_idx = df_local[df_local[col].isna()].index\n",
    "                med_values = df_local.loc[missing_idx, \"__ref_bin__\"].map(grouped_ref)\n",
    "                to_fill = med_values[med_values.notna()]\n",
    "                if not to_fill.empty:\n",
    "                    df_local.loc[to_fill.index, col] = to_fill.values\n",
    "            except Exception:\n",
    "                pass\n",
    "            df_local.drop(columns=[\"__ref_bin__\"], inplace=True, errors=True)\n",
    "    # final fallback: median global if numeric\n",
    "    if is_numeric(df_local[col]) and df_local[col].isna().sum() > 0:\n",
    "        df_local[col] = df_local[col].fillna(df_local[col].median())\n",
    "    n_after = df_local[col].isna().sum()\n",
    "    return n_before - n_after\n",
    "\n",
    "# ---------- Choose critical columns present in df ----------\n",
    "candidate = [\"bmi\",\"systolic_bp\",\"diastolic_bp\",\"ldl\",\"hba1c\",\"income\"]\n",
    "critical_cols = [c for c in candidate if c in df.columns]\n",
    "print(\"Critical columns to impute:\", critical_cols)\n",
    "\n",
    "# ---------- Compute correlation matrix once ----------\n",
    "corr = compute_corr = df.select_dtypes(include=[np.number]).corr().abs()\n",
    "\n",
    "# ---------- Run imputations ----------\n",
    "report = []\n",
    "t0 = time.time()\n",
    "for i, col in enumerate(critical_cols, 1):\n",
    "    t1 = time.time()\n",
    "    filled = impute_col_nested_medians_local(df, col, corr, max_levels=MAX_LEVELS, n_bins=N_BINS, top_k_predictors=TOP_K_PREDICTORS)\n",
    "    t2 = time.time()\n",
    "    report.append((col, filled, df[col].isna().sum(), t2-t1))\n",
    "    print(f\"[{i}/{len(critical_cols)}] {col}: filled={filled}, remaining_nans={df[col].isna().sum()}, time={t2-t1:.1f}s\")\n",
    "\n",
    "print(\"Total time:\", time.time()-t0)\n",
    "\n",
    "# ---------- Save output ----------\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved imputed critical file to:\", OUT_PATH)\n",
    "\n",
    "# ---------- Compare submissions if available ----------\n",
    "def compare_submissions(sub1_path, sub2_path):\n",
    "    try:\n",
    "        s1 = pd.read_csv(sub1_path)\n",
    "        s2 = pd.read_csv(sub2_path)\n",
    "    except Exception as e:\n",
    "        print(\"Submission files not loaded:\", e)\n",
    "        return None\n",
    "    # find id column\n",
    "    id_cols = [c for c in s1.columns if 'id' in c.lower() or 'person' in c.lower()]\n",
    "    if not id_cols:\n",
    "        print(\"No id column found. Returning shapes/cols.\")\n",
    "        return {\"s1_shape\": s1.shape, \"s2_shape\": s2.shape, \"s1_cols\": s1.columns.tolist(), \"s2_cols\": s2.columns.tolist()}\n",
    "    idcol = id_cols[0]\n",
    "    merged = pd.merge(s1, s2, on=idcol, how='inner', suffixes=('_new','_old'))\n",
    "    # find prediction columns (exclude idcol)\n",
    "    preds = [c for c in merged.columns if c!=idcol]\n",
    "    if len(preds) < 2:\n",
    "        # maybe names equal: take first two non-id columns\n",
    "        preds = [c for c in merged.columns if c!=idcol][:2]\n",
    "    pred_new = preds[0]; pred_old = preds[1]\n",
    "    merged['diff'] = merged[pred_new] - merged[pred_old]\n",
    "    stats = {\n",
    "        \"n_rows\": len(merged),\n",
    "        \"mean_old\": merged[pred_old].mean(),\n",
    "        \"mean_new\": merged[pred_new].mean(),\n",
    "        \"mean_diff\": merged['diff'].mean(),\n",
    "        \"median_diff\": merged['diff'].median(),\n",
    "        \"std_diff\": merged['diff'].std(),\n",
    "        \"pct_increase\": (merged['diff']>0).mean(),\n",
    "        \"top5_increases\": merged.sort_values('diff', ascending=False).head(5)[[idcol, pred_old, pred_new, 'diff']].to_dict(orient='records'),\n",
    "        \"top5_decreases\": merged.sort_values('diff').head(5)[[idcol, pred_old, pred_new, 'diff']].to_dict(orient='records')\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "comp = compare_submissions(SUB1_PATH, SUB2_PATH)\n",
    "print(\"Comparison:\", comp)\n",
    "\n",
    "# ---------- Return report DataFrame ----------\n",
    "report_df = pd.DataFrame(report, columns=['col','filled','remaining_nans','time_s'])\n",
    "report_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
